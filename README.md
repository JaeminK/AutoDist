# AutoDist

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.7+-red.svg)
![CUDA](https://img.shields.io/badge/CUDA-Supported-green.svg)

**ìë™ìœ¼ë¡œ ë‹¨ì¼ GPU HuggingFace ëª¨ë¸ì„ ë¶„ì‚° ì¶”ë¡ ì„ ìœ„í•œ í…ì„œ ë³‘ë ¬í™” ë° íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬**

## ğŸ“– Description

AutoDistëŠ” ë‹¨ì¼ GPU HuggingFace ëª¨ë¸ì„ íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ìœ„í•´ í…ì„œ ë³‘ë ¬í™”(Tensor Parallel) ë° íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”(Pipeline Parallel)ë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° ì„¤ì •ìœ¼ë¡œ ìë™ ë³€í™˜í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

### ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **í…ì„œ ë³‘ë ¬í™” (TP)**: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì²˜ë¦¬ ì†ë„ë¥¼ í–¥ìƒ
- **íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” (PP)**: ëª¨ë¸ì˜ ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°í•˜ì—¬ ëŒ€ìš©ëŸ‰ ëª¨ë¸ ì²˜ë¦¬ ê°€ëŠ¥
- **CUDA Graph ìº¡ì²˜**: `ModelWrapper`ë¥¼ í†µí•œ ì„ íƒì  CUDA ê·¸ë˜í”„ ìº¡ì²˜ë¡œ ì»¤ë„ ì‹¤í–‰ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
- **ê°„ë‹¨í•œ CLI**: ë‹¨ì¼ GPU, TP, PP ì¶”ë¡ ì„ ìœ„í•œ ì§ê´€ì ì¸ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤
- **ìë™ ìºì‹±**: TP/PP ìƒ¤ë“œ ë° ìŠ¤í…Œì´ì§€ ìë™ ìƒì„± ë° ìºì‹±

### ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Single GPU    â”‚    â”‚ Tensor Parallel â”‚    â”‚Pipeline Parallelâ”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Model   â”‚  â”‚    â”‚  â”‚ TP1 â”‚ TP2 â”‚  â”‚    â”‚  â”‚ PP1 â”‚ PP2 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Requirements

- **Python**: 3.10 ì´ìƒ
- **PyTorch**: 2.7 ì´ìƒ
- **CUDA**: í˜¸í™˜ ê°€ëŠ¥í•œ PyTorch ë¹Œë“œê°€ í¬í•¨ëœ CUDA ì§€ì› GPU
- **ê¸°íƒ€ ì˜ì¡´ì„±**: `requirements.txt` ì°¸ì¡°

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­

- **ë‹¨ì¼ GPU**: ìµœì†Œ 8GB VRAM (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¦„)
- **í…ì„œ ë³‘ë ¬í™”**: 2ê°œ ì´ìƒì˜ GPU (ë™ì¼í•œ VRAM ê¶Œì¥)
- **íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”**: 2ê°œ ì´ìƒì˜ GPU (ë‹¤ë¥¸ VRAM í¬ê¸° ì§€ì›)

## ğŸ› ï¸ Installation

### 1. ì €ì¥ì†Œ í´ë¡ 

```bash
git clone https://github.com/JaeminK/AutoDist.git
cd AutoDist
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

### 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -e .
```

## ğŸš€ Quick Start

### ë‹¨ì¼ GPU ì¶”ë¡ 

```bash
python main.py \
  --model Qwen/Qwen2.5-7B-Instruct \
  --cache-dir /workspace/cache \
  --output-dir ./results \
  --seed 1234 \
  --min-output-length 1 \
  --max-output-length 512
```

### í…ì„œ ë³‘ë ¬í™” (2ê°œ GPU, 2-way TP)

```bash
torchrun --nproc_per_node 2 main.py \
  --model Qwen/Qwen2.5-7B-Instruct \
  --cache-dir /workspace/cache \
  --output-dir ./results \
  --tensor-parallel-size 2 \
  --seed 1234 \
  --min-output-length 1 \
  --max-output-length 512
```

### íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” (2ê°œ GPU, 2 ìŠ¤í…Œì´ì§€)

```bash
torchrun --nproc_per_node 2 main.py \
  --model Qwen/Qwen2.5-7B-Instruct \
  --cache-dir /workspace/cache \
  --output-dir ./results \
  --pipeline-parallel-size 2 \
  --seed 1234 \
  --min-output-length 1 \
  --max-output-length 512
```

### í…ì„œ + íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” (4ê°œ GPU, 2-way TP + 2 ìŠ¤í…Œì´ì§€ PP)

```bash
torchrun --nproc_per_node 4 main.py \
  --model Qwen/Qwen2.5-7B-Instruct \
  --cache-dir /workspace/cache \
  --output-dir ./results \
  --tensor-parallel-size 2 \
  --pipeline-parallel-size 2 \
  --seed 1234 \
  --min-output-length 1 \
  --max-output-length 512
```

## ğŸ“Š Benchmarks

ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ë‹¨ì¼ GPU ë²¤ì¹˜ë§ˆí¬
cd benchmarks
./test_single.sh

# í…ì„œ ë³‘ë ¬í™” ë²¤ì¹˜ë§ˆí¬ (4 GPU)
./test_tp_dist.sh

# íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” ë²¤ì¹˜ë§ˆí¬ (2 GPU)
./test_pp_dist.sh

# í…ì„œ + íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” ë²¤ì¹˜ë§ˆí¬ (4 GPU)
./test_pp_tp_dist.sh
```


### ì¤‘ìš” ì‚¬í•­

- TP/PP ì‹¤í–‰ ì‹œ `world_size`ëŠ” `tensor_parallel_size * pipeline_parallel_size`ì™€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.
- `local_rank`ëŠ” TPì™€ PP í¬ê¸°ì˜ ê³±ë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.
- `batch_size`ëŠ” ë‹¨ì¼ í† í° ì¶”ë¡ ì„ ìœ„í•´ ë°˜ë“œì‹œ `1`ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ”§ Advanced Usage

### ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš©

```python
from autodist import ModelRunner, DistributedSampler

# ëª¨ë¸ ëŸ¬ë„ˆ ì´ˆê¸°í™”
model_runner = ModelRunner(
    model_name_or_path="your/model/path",
    device="cuda:0",
    dtype=torch.bfloat16,
    tensor_parallel_size=2,
    pipeline_parallel_size=1,
    local_rank=0,
    world_size=2,
    capture_graph=True
)

# ì¶”ë¡  ì‹¤í–‰
output = model_runner.generate(input_ids, max_length=512)
```

### CUDA Graph ìµœì í™”

CUDA Graph ìº¡ì²˜ë¥¼ í™œì„±í™”í•˜ì—¬ ì»¤ë„ ì‹¤í–‰ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
model_runner = ModelRunner(
    # ... ê¸°íƒ€ ì˜µì…˜ë“¤
    capture_graph=True  # CUDA Graph ìº¡ì²˜ í™œì„±í™”
)
```

### ë””ë²„ê¹… íŒ

1. **CUDA ê·¸ë˜í”„ ë¹„í™œì„±í™”**: `capture_graph=False`ë¡œ ì„¤ì •
2. **ë‹¨ì¼ GPU í…ŒìŠ¤íŠ¸**: ë¨¼ì € ë‹¨ì¼ GPUì—ì„œ ëª¨ë¸ì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
3. **ë¡œê·¸ í™•ì¸**: í™˜ê²½ ë³€ìˆ˜ `CUDA_LAUNCH_BLOCKING=1` ì„¤ì •


## ğŸ™ Acknowledgments

- [HuggingFace Transformers](https://github.com/huggingface/transformers) - ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 
- [PyTorch](https://pytorch.org/) - ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - ë¶„ì‚° ì¶”ë¡  ì•„ì´ë””ì–´

## ğŸ“ Contact

- **ì´ìŠˆ ë¦¬í¬íŠ¸**: [GitHub Issues](https://github.com/JaeminK/AutoDist/issues)
- **ì´ë©”ì¼**: [your-email@example.com]
- **í”„ë¡œì íŠ¸ ë§í¬**: [https://github.com/JaeminK/AutoDist](https://github.com/JaeminK/AutoDist)

---

<div align="center">

â­ ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ ìŠ¤íƒ€ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!

</div>
